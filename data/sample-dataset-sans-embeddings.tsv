index	author	title	pages	year	booktitle	source	ee	crossref	url	editor	publisher	series	volume	isbn	cdrom	cite	<cyfunction Entity at 0x10ed74c00>	note	journal	number	abstract	keywords	citation_count	author_processed	keywords_processed	citation_count_processed	abstract_processed	title_processed
0	['Daniel A. Keim', 'Gennady L. Andrienko', 'Jean-Daniel Fekete', 'Carsten G', 'J', 'Guy Melan']	Visual Analytics: Definition, Process, and Challenges.	154-175	2008	Information Visualization	Information Visualization	['https://doi.org/10.1007/978-3-540-70956-5_7']	series/lncs/4950	['db/series/lncs/lncs4950.html#KeimAFGKM08']												We are living in a world which faces a rapidly increasing amount of data to be dealt with on a daily basis. In the last decade, the steady improvement of data storage devices and means to create and collect data along the way influenced our way of dealing with information: Most of the time, data is stored without filtering and refinement for later use. Virtually every branch of industry or business, and any political or personal activity nowadays generate vast amounts of data. Making matters worse, the possibilities to collect and store data increase at a faster rate than our ability to use it for making decisions. However, in most applications, raw data has no value in itself; instead we want to extract the information contained in it.	['Information Visualization', 'Knowledge Discovery Process', 'Interaction Technique', 'Visual Analytic', 'Analysis Task']	303	['Daniel A. Keim', 'Gennady L. Andrienko', 'Jean-daniel Fekete', 'Carsten G', 'J', 'Guy Melan']	['Analysis Task', 'Information Visualization', 'Interaction Technique', 'Knowledge Discovery Process', 'Visual Analytic']	303.0	We are living in a world which faces a rapidly increasing amount of data to be dealt with on a daily basis. In the last decade, the steady improvement of data storage devices and means to create and collect data along the way influenced our way of dealing with information: Most of the time, data is stored without filtering and refinement for later use. Virtually every branch of industry or business, and any political or personal activity nowadays generate vast amounts of data. Making matters worse, the possibilities to collect and store data increase at a faster rate than our ability to use it for making decisions. However, in most applications, raw data has no value in itself; instead we want to extract the information contained in it.	Visual Analytics: Definition, Process, and Challenges.
1	['Jeffrey Heer', 'Frank van Ham', 'Sheelagh Carpendale', 'Chris Weaver', 'Petra Isenberg']	Creation and Collaboration: Engaging New Audiences for Information Visualization.	92-133	2008	Information Visualization	Information Visualization	['https://doi.org/10.1007/978-3-540-70956-5_5']	series/lncs/4950	['db/series/lncs/lncs4950.html#HeerHCWI08']												In recent years we have seen information visualization technology move from an advanced research topic to mainstream adoption in both commercial and personal use. This move is in part due to many businesses recognizing the need for more effective tools for extracting knowledge from the data warehouses they are gathering. Increased mainstream interest is also a result of more exposure to advanced interfaces in contemporary online media. The adoption of information visualization technologies by lay users – as opposed to the traditional information visualization audience of scientists and analysts – has important implications for visualization research, design and development. Since we cannot expect each of these lay users to design their own visualizations, we have to provide them tools that make it easy to create and deploy visualizations of their datasets.	['Information Visualization', 'Visualization System', 'IEEE Computer Society', 'Interactive Visualization', 'Novice User']	39	['Jeffrey Heer', 'Frank Van Ham', 'Sheelagh Carpendale', 'Chris Weaver', 'Petra Isenberg']	['Information Visualization', 'Ieee Computer Society', 'Novice User', 'Visualization System', 'Interactive Visualization']	39.0	In recent years we have seen information visualization technology move from an advanced research topic to mainstream adoption in both commercial and personal use. This move is in part due to many businesses recognizing the need for more effective tools for extracting knowledge from the data warehouses they are gathering. Increased mainstream interest is also a result of more exposure to advanced interfaces in contemporary online media. The adoption of information visualization technologies by lay users – as opposed to the traditional information visualization audience of scientists and analysts – has important implications for visualization research, design and development. Since we cannot expect each of these lay users to design their own visualizations, we have to provide them tools that make it easy to create and deploy visualizations of their datasets.	Creation and Collaboration: Engaging New Audiences for Information Visualization.
2	['Helen C. Purchase', 'Natalia V. Andrienko', 'T. J. Jankun-Kelly', 'Matthew O. Ward']	Theoretical Foundations of Information Visualization.	46-64	2008	Information Visualization	Information Visualization	['https://doi.org/10.1007/978-3-540-70956-5_3', 'https://www.wikidata.org/entity/Q58203993']	series/lncs/4950	['db/series/lncs/lncs4950.html#PurchaseAJW08']												The field of Information Visualization, being related to many other diverse disciplines (for example, engineering, graphics, statistical modeling) suffers from not being based on a clear underlying theory. The absence of a framework for Information Visualization makes the significance of achievements in this area difficult to describe, validate and defend. Drawing on theories within associated disciplines, three different approaches to theoretical foundations of Information Visualization are presented here: data-centric predictive theory, information theory, and scientific modeling. Definitions from linguistic theory are used to provide an over-arching framework for these three approaches.	['Information Visualization', 'Composite Pattern', 'IEEE Symposium', 'Formal Foundation', 'IEEE Computer Society']	31	['Helen C. Purchase', 'Natalia V. Andrienko', 'T. J. Jankun-kelly', 'Matthew O. Ward']	['Information Visualization', 'Ieee Computer Society', 'Ieee Symposium', 'Composite Pattern', 'Formal Foundation']	31.0	The field of Information Visualization, being related to many other diverse disciplines (for example, engineering, graphics, statistical modeling) suffers from not being based on a clear underlying theory. The absence of a framework for Information Visualization makes the significance of achievements in this area difficult to describe, validate and defend. Drawing on theories within associated disciplines, three different approaches to theoretical foundations of Information Visualization are presented here: data-centric predictive theory, information theory, and scientific modeling. Definitions from linguistic theory are used to provide an over-arching framework for these three approaches.	Theoretical Foundations of Information Visualization.
3	['Jean-Daniel Fekete', 'Jarke J. van Wijk', 'John T. Stasko', 'Chris North']	The Value of Information Visualization.	Jan-18	2008	Information Visualization	Information Visualization	['https://doi.org/10.1007/978-3-540-70956-5_1', 'https://www.wikidata.org/entity/Q56679313']	series/lncs/4950	['db/series/lncs/lncs4950.html#FeketeWSN08']												Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.	['Information Visualization', 'Perceptual Chunk', 'Perceptual Inference', 'Preattentive Processing', 'Exploratory Data Analysis']	61	['Jean-daniel Fekete', 'Jarke J. Van Wijk', 'John T. Stasko', 'Chris North']	['Exploratory Data Analysis', 'Information Visualization', 'Perceptual Inference', 'Preattentive Processing', 'Perceptual Chunk']	61.0	Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.	The Value of Information Visualization.
5	['Tamara Munzner']	Process and Pitfalls in Writing Information Visualization Research Papers.	134-153	2008	Information Visualization	Information Visualization	['https://doi.org/10.1007/978-3-540-70956-5_6']	series/lncs/4950	['db/series/lncs/lncs4950.html#Munzner08']												The goal of this chapter is to help authors recognize and avoid a set of pitfalls that recur in many rejected information visualization papers, using a chronological model of the research process. Selecting a target paper type in the initial stage can avert an inappropriate choice of validation methods. Pitfalls involving the design of a visual encoding may occur during the middle stages of a project. In a later stage when the bulk of the research is finished and the paper writeup begins, the possible pitfalls are strategic choices for the content and structure of the paper as a whole, tactical problems localized to specific sections, and unconvincing ways to present the results. Final-stage pitfalls of writing style can be checked after a full paper draft exists, and the last set of problems pertain to submission.	['Target User', 'Information Visualization', 'IEEE Symposium', 'Paper Type', 'Research Contribution']	31	['Tamara Munzner']	['Information Visualization', 'Target User', 'Ieee Symposium', 'Paper Type', 'Research Contribution']	31.0	The goal of this chapter is to help authors recognize and avoid a set of pitfalls that recur in many rejected information visualization papers, using a chronological model of the research process. Selecting a target paper type in the initial stage can avert an inappropriate choice of validation methods. Pitfalls involving the design of a visual encoding may occur during the middle stages of a project. In a later stage when the bulk of the research is finished and the paper writeup begins, the possible pitfalls are strategic choices for the content and structure of the paper as a whole, tactical problems localized to specific sections, and unconvincing ways to present the results. Final-stage pitfalls of writing style can be checked after a full paper draft exists, and the last set of problems pertain to submission.	Process and Pitfalls in Writing Information Visualization Research Papers.
6	['Sheelagh Carpendale']	Evaluating Information Visualizations.	19-45	2008	Information Visualization	Information Visualization	['https://doi.org/10.1007/978-3-540-70956-5_2']	series/lncs/4950	['db/series/lncs/lncs4950.html#Carpendale08']												Information visualization research is becoming more established, and as a result, it is becoming increasingly important that research in this field is validated. With the general increase in information visualization research there has also been an increase, albeit disproportionately small, in the amount of empirical work directly focused on information visualization. The purpose of this chapter is to increase awareness of empirical research in general, of its relationship to information visualization in particular; to emphasize its importance; and to encourage thoughtful application of a greater variety of evaluative research methodologies in information visualization.	['Information Visualization', 'Empirical Research', 'Qualitative Method', 'Sage Publication', 'Visualization Technique']	127	['Sheelagh Carpendale']	['Information Visualization', 'Empirical Research', 'Visualization Technique', 'Qualitative Method', 'Sage Publication']	127.0	Information visualization research is becoming more established, and as a result, it is becoming increasingly important that research in this field is validated. With the general increase in information visualization research there has also been an increase, albeit disproportionately small, in the amount of empirical work directly focused on information visualization. The purpose of this chapter is to increase awareness of empirical research in general, of its relationship to information visualization in particular; to emphasize its importance; and to encourage thoughtful application of a greater variety of evaluative research methodologies in information visualization.	Evaluating Information Visualizations.
7	['Andreas Kerren', 'John T. Stasko', 'Jason Dykes']	Teaching Information Visualization.	65-91	2008	Information Visualization	Information Visualization	['https://doi.org/10.1007/978-3-540-70956-5_4']	series/lncs/4950	['db/series/lncs/lncs4950.html#KerrenSD08']												Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.	['Georgia Tech', 'Information Visualization', 'Interaction Technique', 'Graph Drawing', 'Visualization Technique']	12	['Andreas Kerren', 'John T. Stasko', 'Jason Dykes']	['Information Visualization', 'Graph Drawing', 'Interaction Technique', 'Visualization Technique', 'Georgia Tech']	12.0	Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.	Teaching Information Visualization.
7362	['Fraser Anderson', 'Tovi Grossman', 'Justin Matejka', 'George W. Fitzmaurice']	YouMove: enhancing movement training with an augmented reality mirror.	311-320	2013	UIST	UIST	['https://doi.org/10.1145/2501988.2502045']	conf/uist/2013	['db/conf/uist/uist2013.html#AndersonGMF13']												YouMove is a novel system that allows users to record and learn physical movement sequences. The recording system is designed to be simple, allowing anyone to create and share training content. The training system uses recorded data to train the user using a large-scale augmented reality mirror. The system trains the user through a series of stages that gradually reduce the user's reliance on guidance and feedback. This paper discusses the design and implementation of YouMove and its interactive mirror. We also present a user study in which YouMove was shown to improve learning and short-term retention by a factor of 2 compared to a traditional video demonstration.	['Graphical user interfaces', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms']	123	['Fraser Anderson', 'Tovi Grossman', 'Justin Matejka', 'George W. Fitzmaurice']	['Graphical User Interfaces', 'Human Computer Interaction', 'Interaction Paradigms', 'Human-centered Computing']	123.0	YouMove is a novel system that allows users to record and learn physical movement sequences. The recording system is designed to be simple, allowing anyone to create and share training content. The training system uses recorded data to train the user using a large-scale augmented reality mirror. The system trains the user through a series of stages that gradually reduce the user's reliance on guidance and feedback. This paper discusses the design and implementation of YouMove and its interactive mirror. We also present a user study in which YouMove was shown to improve learning and short-term retention by a factor of 2 compared to a traditional video demonstration.	YouMove: enhancing movement training with an augmented reality mirror.
7363	['Masa Ogata']	Magneto-Haptics: Embedding Magnetic Force Feedback for Physical Interactions.	737-743	2018	UIST	UIST	['https://doi.org/10.1145/3242587.3242615']	conf/uist/2018	['db/conf/uist/uist2018.html#Ogata18']												We present magneto-haptics, a design approach of haptic sensations powered by the forces present among permanent magnets during active touch. Magnetic force has not been efficiently explored in haptic design because it is not intuitive and there is a lack of methods to associate or visualize magnetic force with haptic sensations, especially for complex magnetic patterns. To represent the haptic sensations of magnetic force intuitively, magneto-haptics formularizes haptic potential from the distribution of magnetic force along the path of motion. It provides a rapid way to compute the relationship between the magnetic phenomena and the haptic mechanism. Thus, we can convert a magnetic force distribution into a haptic sensation model, making the design of magnet-embedded haptic sensations more efficient. We demonstrate three applications of magneto-haptics through interactive interfaces and devices. We further verify our theory by evaluating some magneto-haptic designs through experiments.	['Virtual reality', 'Interaction devices', 'Haptic devices', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms']	10	['Masa Ogata']	['Virtual Reality', 'Human-centered Computing', 'Interaction Devices', 'Human Computer Interaction', 'Interaction Paradigms', 'Haptic Devices']	10.0	We present magneto-haptics, a design approach of haptic sensations powered by the forces present among permanent magnets during active touch. Magnetic force has not been efficiently explored in haptic design because it is not intuitive and there is a lack of methods to associate or visualize magnetic force with haptic sensations, especially for complex magnetic patterns. To represent the haptic sensations of magnetic force intuitively, magneto-haptics formularizes haptic potential from the distribution of magnetic force along the path of motion. It provides a rapid way to compute the relationship between the magnetic phenomena and the haptic mechanism. Thus, we can convert a magnetic force distribution into a haptic sensation model, making the design of magnet-embedded haptic sensations more efficient. We demonstrate three applications of magneto-haptics through interactive interfaces and devices. We further verify our theory by evaluating some magneto-haptic designs through experiments.	Magneto-Haptics: Embedding Magnetic Force Feedback for Physical Interactions.
7364	['Ayaka Ishii', 'Itiro Siio']	BubBowl: Display Vessel Using Electrolysis Bubbles in Drinkable Beverages.	619-623	2019	UIST	UIST	['https://doi.org/10.1145/3332165.3347923']	conf/uist/2019	['db/conf/uist/uist2019.html#IshiiS19']												Research was conducted regarding a display that presents digital information using bubbles. Conventional bubble displays require moving parts, because it is common to use air taken from outside of the water to represent pixels. However, it is difficult to increase the number of pixels at a low cost. We propose a liquid-surface display using pixels of bubble clusters generated from electrolysis, and present the cup-type device BubBowl, which generates a 10×10 pixel dot matrix pattern on the surface of a beverage. Our technique requires neither a gas supply from the outside nor moving parts. Using the proposed electrolysis method, a higher-resolution display can easily be realized using a PCB with a higher density of matrix electrodes.Moreover, the method is simple and practical, and can be utilized in daily life, such as for presenting information using bubbles on the surface of coffee in a cup.	['Human computer interaction (HCI)', 'Human-centered computing']	4	['Ayaka Ishii', 'Itiro Siio']	['Human Computer Interaction', 'Human-centered Computing']	4.0	Research was conducted regarding a display that presents digital information using bubbles. Conventional bubble displays require moving parts, because it is common to use air taken from outside of the water to represent pixels. However, it is difficult to increase the number of pixels at a low cost. We propose a liquid-surface display using pixels of bubble clusters generated from electrolysis, and present the cup-type device BubBowl, which generates a 10×10 pixel dot matrix pattern on the surface of a beverage. Our technique requires neither a gas supply from the outside nor moving parts. Using the proposed electrolysis method, a higher-resolution display can easily be realized using a PCB with a higher density of matrix electrodes.Moreover, the method is simple and practical, and can be utilized in daily life, such as for presenting information using bubbles on the surface of coffee in a cup.	BubBowl: Display Vessel Using Electrolysis Bubbles in Drinkable Beverages.
7365	['Gareth Barnaby', 'Anne Roudaut']	Mantis: A Scalable, Lightweight and Accessible Architecture to Build Multiform Force Feedback Systems.	937-948	2019	UIST	UIST	['https://doi.org/10.1145/3332165.3347909']	conf/uist/2019	['db/conf/uist/uist2019.html#BarnabyR19']												Mantis is a highly scalable system architecture that democratizes haptic devices by enabling designers to create accurate, multiform and accessible force feedback systems. Mantis uses brushless DC motors, custom electronic controllers, and an admittance control scheme to achieve stable high-quality haptic rendering. It enables common desktop form factors but also: large workspaces (multiple arm lengths), multiple arm workspaces, and mobile workspaces. It also uses accessible components and costs significantly less than typical high-fidelity force feedback solutions which are often confined to research labs. We present our design and show that Mantis can reproduce the haptic fidelity of common robotic arms. We demonstrate its multiform ability by implementing five systems: a single desktop-sized device, a single large workspace device, a large workspace system with four points of feedback, a mobile system and a wearable one.	['Communication hardware', 'Tactile and hand-based interfaces', 'Haptic devices', 'Hardware']	3	['Gareth Barnaby', 'Anne Roudaut']	['Hardware', 'Tactile And Hand-based Interfaces', 'Haptic Devices', 'Communication Hardware']	3.0	Mantis is a highly scalable system architecture that democratizes haptic devices by enabling designers to create accurate, multiform and accessible force feedback systems. Mantis uses brushless DC motors, custom electronic controllers, and an admittance control scheme to achieve stable high-quality haptic rendering. It enables common desktop form factors but also: large workspaces (multiple arm lengths), multiple arm workspaces, and mobile workspaces. It also uses accessible components and costs significantly less than typical high-fidelity force feedback solutions which are often confined to research labs. We present our design and show that Mantis can reproduce the haptic fidelity of common robotic arms. We demonstrate its multiform ability by implementing five systems: a single desktop-sized device, a single large workspace device, a large workspace system with four points of feedback, a mobile system and a wearable one.	Mantis: A Scalable, Lightweight and Accessible Architecture to Build Multiform Force Feedback Systems.
7366	['Yasuaki Monnai', 'Keisuke Hasegawa', 'Masahiro Fujiwara', 'Kazuma Yoshino', 'Seki Inoue', 'Hiroyuki Shinoda']	HaptoMime: mid-air haptic interaction with a floating virtual screen.	663-667	2014	UIST	UIST	['https://doi.org/10.1145/2642918.2647407']	conf/uist/2014	['db/conf/uist/uist2014.html#MonnaiHFYIS14']												We present HaptoMime, a mid-air interaction system that allows users to touch a floating virtual screen with hands-free tactile feedback. Floating images formed by tailored light beams are inherently lacking in tactile feedback. Here we propose a method to superpose hands-free tactile feedback on such a floating image using ultrasound. By tracking a fingertip with an electronically steerable ultrasonic beam, the fingertip encounters a mechanical force consistent with the floating image. We demonstrate and characterize the proposed transmission scheme and discuss promising applications with an emphasis that it helps us 'pantomime' in mid-air.	['Human computer interaction (HCI)', 'Human-centered computing', 'Interaction devices', 'Haptic devices']	57	['Yasuaki Monnai', 'Keisuke Hasegawa', 'Masahiro Fujiwara', 'Kazuma Yoshino', 'Seki Inoue', 'Hiroyuki Shinoda']	['Interaction Devices', 'Human Computer Interaction', 'Haptic Devices', 'Human-centered Computing']	57.0	We present HaptoMime, a mid-air interaction system that allows users to touch a floating virtual screen with hands-free tactile feedback. Floating images formed by tailored light beams are inherently lacking in tactile feedback. Here we propose a method to superpose hands-free tactile feedback on such a floating image using ultrasound. By tracking a fingertip with an electronically steerable ultrasonic beam, the fingertip encounters a mechanical force consistent with the floating image. We demonstrate and characterize the proposed transmission scheme and discuss promising applications with an emphasis that it helps us 'pantomime' in mid-air.	HaptoMime: mid-air haptic interaction with a floating virtual screen.
7367	['Andrew Wilson', 'Hrvoje Benko', 'Shahram Izadi', 'Otmar Hilliges']	Steerable augmented reality with the beamatron.	413-422	2012	UIST	UIST	['https://doi.org/10.1145/2380116.2380169']	conf/uist/2012	['db/conf/uist/uist2012.html#WilsonBIH12']												Steerable displays use a motorized platform to orient a projector to display graphics at any point in the room. Often a camera is included to recognize markers and other objects, as well as user gestures in the display volume. Such systems can be used to superimpose graphics onto the real world, and so are useful in a number of augmented reality and ubiquitous computing scenarios. We contribute the Beamatron, which advances steerable displays by drawing on recent progress in depth camera-based interactions. The Beamatron consists of a computer-controlled pan and tilt platform on which is mounted a projector and Microsoft Kinect sensor. While much previous work with steerable displays deals primarily with projecting corrected graphics onto a discrete set of static planes, we describe computational techniques that enable reasoning in 3D using live depth data. We show two example applications that are enabled by the unique capabilities of the Beamatron: an augmented reality game in which a player can drive a virtual toy car around a room, and a ubiquitous computing demo that uses speech and gesture to move projected graphics throughout the room.	['Human computer interaction (HCI)', 'Human-centered computing']	70	['Andrew Wilson', 'Hrvoje Benko', 'Shahram Izadi', 'Otmar Hilliges']	['Human Computer Interaction', 'Human-centered Computing']	70.0	Steerable displays use a motorized platform to orient a projector to display graphics at any point in the room. Often a camera is included to recognize markers and other objects, as well as user gestures in the display volume. Such systems can be used to superimpose graphics onto the real world, and so are useful in a number of augmented reality and ubiquitous computing scenarios. We contribute the Beamatron, which advances steerable displays by drawing on recent progress in depth camera-based interactions. The Beamatron consists of a computer-controlled pan and tilt platform on which is mounted a projector and Microsoft Kinect sensor. While much previous work with steerable displays deals primarily with projecting corrected graphics onto a discrete set of static planes, we describe computational techniques that enable reasoning in 3D using live depth data. We show two example applications that are enabled by the unique capabilities of the Beamatron: an augmented reality game in which a player can drive a virtual toy car around a room, and a ubiquitous computing demo that uses speech and gesture to move projected graphics throughout the room.	Steerable augmented reality with the beamatron.
7368	['Lawrence D. Bergman', 'Vittorio Castelli', 'Tessa A. Lau', 'Daniel Oblinger']	DocWizards: a system for authoring follow-me documentation wizards.	191-200	2005	UIST	UIST	['https://doi.org/10.1145/1095034.1095067']	conf/uist/2005	['db/conf/uist/uist2005.html#BergmanCLO05']												Traditional documentation for computer-based procedures is difficult to use: readers have trouble navigating long complex instructions, have trouble mapping from the text to display widgets, and waste time performing repetitive procedures. We propose a new class of improved documentation that we call follow-me documentation wizards. Follow-me documentation wizards step a user through a script representation of a procedure by highlighting portions of the text, as well application UI elements. This paper presents algorithms for automatically capturing follow-me documentation wizards by demonstration, through observing experts performing the procedure. We also present our DocWizards implementation on the Eclipse platform. We evaluate our system with an initial user study that showing that most users have a marked preference for this form of guidance over traditional documentation.	['Human computer interaction (HCI)', 'Human-centered computing']	68	['Lawrence D. Bergman', 'Vittorio Castelli', 'Tessa A. Lau', 'Daniel Oblinger']	['Human Computer Interaction', 'Human-centered Computing']	68.0	Traditional documentation for computer-based procedures is difficult to use: readers have trouble navigating long complex instructions, have trouble mapping from the text to display widgets, and waste time performing repetitive procedures. We propose a new class of improved documentation that we call follow-me documentation wizards. Follow-me documentation wizards step a user through a script representation of a procedure by highlighting portions of the text, as well application UI elements. This paper presents algorithms for automatically capturing follow-me documentation wizards by demonstration, through observing experts performing the procedure. We also present our DocWizards implementation on the Eclipse platform. We evaluate our system with an initial user study that showing that most users have a marked preference for this form of guidance over traditional documentation.	DocWizards: a system for authoring follow-me documentation wizards.
7369	"['Camilo Fosco', 'Vincent Casser', 'Amish Kumar Bedi', ""Peter O'Donovan"", 'Aaron Hertzmann', 'Zoya Bylinskii']"	Predicting Visual Importance Across Graphic Design Types.	249-260	2020	UIST	UIST	['https://doi.org/10.1145/3379337.3415825']	conf/uist/2020	['db/conf/uist/uist2020.html#FoscoCBOHB20']												This paper introduces a Unified Model of Saliency and Importance (UMSI), which learns to predict visual importance in input graphic designs, and saliency in natural images, along with a new dataset and applications. Previous methods for predicting saliency or visual importance are trained individually on specialized datasets, making them limited in application and leading to poor generalization on novel image classes, while requiring a user to know which model to apply to which input. UMSI is a deep learning-based model simultaneously trained on images from different design classes, including posters, infographics, mobile UIs, as well as natural images, and includes an automatic classification module to classify the input. This allows the model to work more effectively without requiring a user to label the input. We also introduce Imp1k, a new dataset of designs annotated with importance information. We demonstrate two new design interfaces that use importance prediction, including a tool for adjusting the relative importance of design elements, and a tool for reflowing designs to new aspect ratios while preserving visual importance.	['Machine learning', 'Multimedia information systems', 'Interest point and salient region detections', 'Information systems', 'Computer vision', 'Scene understanding', 'Information systems applications', 'Computing methodologies', 'Computer vision problems', 'Machine learning approaches', 'Artificial intelligence', 'Neural networks', 'Computer vision tasks']	0	"['Camilo Fosco', 'Vincent Casser', 'Amish Kumar Bedi', ""Peter O'donovan"", 'Aaron Hertzmann', 'Zoya Bylinskii']"	['Multimedia Information Systems', 'Scene Understanding', 'Computing Methodologies', 'Computer Vision', 'Information Systems', 'Computer Vision Problems', 'Artificial Intelligence', 'Information Systems Applications', 'Computer Vision Tasks', 'Machine Learning Approaches', 'Neural Networks', 'Machine Learning', 'Interest Point And Salient Region Detections']	0.0	This paper introduces a Unified Model of Saliency and Importance (UMSI), which learns to predict visual importance in input graphic designs, and saliency in natural images, along with a new dataset and applications. Previous methods for predicting saliency or visual importance are trained individually on specialized datasets, making them limited in application and leading to poor generalization on novel image classes, while requiring a user to know which model to apply to which input. UMSI is a deep learning-based model simultaneously trained on images from different design classes, including posters, infographics, mobile UIs, as well as natural images, and includes an automatic classification module to classify the input. This allows the model to work more effectively without requiring a user to label the input. We also introduce Imp1k, a new dataset of designs annotated with importance information. We demonstrate two new design interfaces that use importance prediction, including a tool for adjusting the relative importance of design elements, and a tool for reflowing designs to new aspect ratios while preserving visual importance.	Predicting Visual Importance Across Graphic Design Types.
7370	['Joanne Lo', 'Eric Paulos']	ShrinkyCircuits: sketching, shrinking, and formgiving for electronic circuits.	291-299	2014	UIST	UIST	['https://doi.org/10.1145/2642918.2647421']	conf/uist/2014	['db/conf/uist/uist2014.html#LoP14']												In this paper we describe the development of ShrinkyCircuits, a novel electronic prototyping technique that captures the flexibility of sketching and leverages properties of a common everyday plastic polymer to enable low-cost, miniature, planar, and curved, multi-layer circuit designs in minutes. ShrinkyCircuits take advantage of inexpensive prestressed polymer film that shrinks to its original size when exposed to heat. This enables improved electrical characteristics though sintering of the conductive electrical layer, partial self-assembly of the circuit and components, and mechanically robust custom shapes Including curves and non-planar form factors. We demonstrate the range and adaptability of ShrinkyCircuits designs from simple hand drawn circuits with through-hole components to complex multilayer, printed circuit boards (PCB), with curved and irregular shaped electronic layouts and surface mount components. Our approach enables users to create extremely customized circuit boards with dense circuit layouts while avoiding messy chemical etching, expensive board milling machines, or time consuming delays in using outside PCB production houses.	['Human-centered computing']	17	['Joanne Lo', 'Eric Paulos']	['Human-centered Computing']	17.0	In this paper we describe the development of ShrinkyCircuits, a novel electronic prototyping technique that captures the flexibility of sketching and leverages properties of a common everyday plastic polymer to enable low-cost, miniature, planar, and curved, multi-layer circuit designs in minutes. ShrinkyCircuits take advantage of inexpensive prestressed polymer film that shrinks to its original size when exposed to heat. This enables improved electrical characteristics though sintering of the conductive electrical layer, partial self-assembly of the circuit and components, and mechanically robust custom shapes Including curves and non-planar form factors. We demonstrate the range and adaptability of ShrinkyCircuits designs from simple hand drawn circuits with through-hole components to complex multilayer, printed circuit boards (PCB), with curved and irregular shaped electronic layouts and surface mount components. Our approach enables users to create extremely customized circuit boards with dense circuit layouts while avoiding messy chemical etching, expensive board milling machines, or time consuming delays in using outside PCB production houses.	ShrinkyCircuits: sketching, shrinking, and formgiving for electronic circuits.
7371	['T. Scott Saponas', 'Chris Harrison 0001', 'Hrvoje Benko']	PocketTouch: through-fabric capacitive touch input.	303-308	2011	UIST	UIST	['https://doi.org/10.1145/2047196.2047235']	conf/uist/2011	['db/conf/uist/uist2011.html#SaponasHB11']												PocketTouch is a capacitive sensing prototype that enables eyes-free multitouch input on a handheld device without having to remove the device from the pocket of one's pants, shirt, bag, or purse. PocketTouch enables a rich set of gesture interactions, ranging from simple touch strokes to full alphanumeric text entry. Our prototype device consists of a custom multitouch capacitive sensor mounted on the back of a smartphone. Similar capabilities could be enabled on most existing capacitive touchscreens through low-level access to the capacitive sensor. We demonstrate how touch strokes can be used to initialize the device for interaction and how strokes can be processed to enable text recognition of characters written over the same physical area. We also contribute a comparative study that empirically measures how different fabrics attenuate touch inputs, providing insight for future investigations. Our results suggest that PocketTouch will work reliably with a wide variety of fabrics used in today's garments, and is a viable input method for quick eyes-free operation of devices in pockets.	['Human computer interaction (HCI)', 'Human-centered computing']	60	['T. Scott Saponas', 'Chris Harrison 0001', 'Hrvoje Benko']	['Human Computer Interaction', 'Human-centered Computing']	60.0	PocketTouch is a capacitive sensing prototype that enables eyes-free multitouch input on a handheld device without having to remove the device from the pocket of one's pants, shirt, bag, or purse. PocketTouch enables a rich set of gesture interactions, ranging from simple touch strokes to full alphanumeric text entry. Our prototype device consists of a custom multitouch capacitive sensor mounted on the back of a smartphone. Similar capabilities could be enabled on most existing capacitive touchscreens through low-level access to the capacitive sensor. We demonstrate how touch strokes can be used to initialize the device for interaction and how strokes can be processed to enable text recognition of characters written over the same physical area. We also contribute a comparative study that empirically measures how different fabrics attenuate touch inputs, providing insight for future investigations. Our results suggest that PocketTouch will work reliably with a wide variety of fabrics used in today's garments, and is a viable input method for quick eyes-free operation of devices in pockets.	PocketTouch: through-fabric capacitive touch input.
7372	['Artem Dementyev', 'Tom', 'Alex Olwal']	SensorSnaps: Integrating Wireless Sensor Nodes into Fabric Snap Fasteners for Textile Interfaces.	17-28	2019	UIST	UIST	['https://doi.org/10.1145/3332165.3347913']	conf/uist/2019	['db/conf/uist/uist2019.html#DementyevGO19']												Adding electronics to textiles can be time-consuming and requires technical expertise. We introduce SensorSnaps, low-power wireless sensor nodes that seamlessly integrate into caps of fabric snap fasteners. SensorSnaps provide a new technique to quickly and intuitively augment any location on the clothing with sensing capabilities. SensorSnaps securely attach and detach from ubiquitous commercial snap fasteners. Using inertial measurement units, the SensorSnaps detect tap and rotation gestures, as well as track body motion. We optimized the power consumption for SensorSnaps to work continuously for 45 minutes and up to 4 hours in capacitive touch standby mode. We present applications in which the SensorSnaps are used as gestural interfaces for a music player controller, cursor control, and motion tracking suit. The user study showed that SensorSnap could be attached in around 71 seconds, similar to attaching off-the-shelf snaps, and participants found the gestures easy to learn and perform. SensorSnaps could allow anyone to effortlessly add sophisticated sensing capacities to ubiquitous snap fasteners.	['Interaction techniques', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction devices']	5	['Artem Dementyev', 'Tom', 'Alex Olwal']	['Interaction Devices', 'Human Computer Interaction', 'Human-centered Computing', 'Interaction Techniques']	5.0	Adding electronics to textiles can be time-consuming and requires technical expertise. We introduce SensorSnaps, low-power wireless sensor nodes that seamlessly integrate into caps of fabric snap fasteners. SensorSnaps provide a new technique to quickly and intuitively augment any location on the clothing with sensing capabilities. SensorSnaps securely attach and detach from ubiquitous commercial snap fasteners. Using inertial measurement units, the SensorSnaps detect tap and rotation gestures, as well as track body motion. We optimized the power consumption for SensorSnaps to work continuously for 45 minutes and up to 4 hours in capacitive touch standby mode. We present applications in which the SensorSnaps are used as gestural interfaces for a music player controller, cursor control, and motion tracking suit. The user study showed that SensorSnap could be attached in around 71 seconds, similar to attaching off-the-shelf snaps, and participants found the gestures easy to learn and perform. SensorSnaps could allow anyone to effortlessly add sophisticated sensing capacities to ubiquitous snap fasteners.	SensorSnaps: Integrating Wireless Sensor Nodes into Fabric Snap Fasteners for Textile Interfaces.
7374	['Adiyan Mujibiya', 'Jun Rekimoto']	Mirage: exploring interaction modalities using off-body static electric field sensing.	211-220	2013	UIST	UIST	['https://doi.org/10.1145/2501988.2502031']	conf/uist/2013	['db/conf/uist/uist2013.html#MujibiyaR13']												Mirage proposes an effective non body contact technique to infer the amount and type of body motion, gesture, and activity. This approach involves passive measurement of static electric field of the environment flowing through sense electrode. This sensing method leverages electric field distortion by the presence of an intruder (e.g. human body). Mirage sensor has simple analog circuitry and supports ultra-low power operation. It requires no instrumentation to the user, and can be configured as environmental, mobile, and peripheral-attached sensor. We report on a series of experiments with 10 participants showing robust activity and gesture recognition, as well as promising results for robust location classification and multiple user differentiation. To further illustrate the utility of our approach, we demonstrate real-time interactive applications including activity monitoring, and two games which allow the users to interact with a computer using body motion and gestures.	['Touch screens', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction devices']	17	['Adiyan Mujibiya', 'Jun Rekimoto']	['Interaction Devices', 'Human Computer Interaction', 'Touch Screens', 'Human-centered Computing']	17.0	Mirage proposes an effective non body contact technique to infer the amount and type of body motion, gesture, and activity. This approach involves passive measurement of static electric field of the environment flowing through sense electrode. This sensing method leverages electric field distortion by the presence of an intruder (e.g. human body). Mirage sensor has simple analog circuitry and supports ultra-low power operation. It requires no instrumentation to the user, and can be configured as environmental, mobile, and peripheral-attached sensor. We report on a series of experiments with 10 participants showing robust activity and gesture recognition, as well as promising results for robust location classification and multiple user differentiation. To further illustrate the utility of our approach, we demonstrate real-time interactive applications including activity monitoring, and two games which allow the users to interact with a computer using body motion and gestures.	Mirage: exploring interaction modalities using off-body static electric field sensing.
7375	['Jaime Teevan']	The re: search engine: simultaneous support for finding and re-finding.	23-32	2007	UIST	UIST	['https://doi.org/10.1145/1294211.1294217']	conf/uist/2007	['db/conf/uist/uist2007.html#Teevan07']												"Re-finding, a common Web task, is difficult when previously viewed information is modified, moved, or removed. For example, if a person finds a good result using the query ""breast cancer treatments"", she expects to be able to use the same query to locate the same result again. While re-finding could be supported by caching the original list, caching precludes the discovery of new information, such as, in this case, new treatment options. People often use search engines to simultaneously find and re-find information. The Re:Search Engine is designed to support both behaviors in dynamic environments like the Web by preserving only the memorable aspects of a result list. A study of result list memory shows that people forget a lot. The Re:Search Engine takes advantage of these memory lapses to include new results where old results have been forgotten."	['Document preparation', 'Multi / mixed media creation', 'Hypertext / hypermedia', 'Graphical user interfaces', 'Document management and text processing', 'Human computer interaction (HCI)', 'Applied computing', 'Human-centered computing', 'Interaction paradigms']	30	['Jaime Teevan']	['Graphical User Interfaces', 'Human-centered Computing', 'Hypertext Hypermedia', 'Document Management And Text Processing', 'Applied Computing', 'Document Preparation', 'Human Computer Interaction', 'Interaction Paradigms', 'Multi Mixed Media Creation']	30.0	"Re-finding, a common Web task, is difficult when previously viewed information is modified, moved, or removed. For example, if a person finds a good result using the query ""breast cancer treatments"", she expects to be able to use the same query to locate the same result again. While re-finding could be supported by caching the original list, caching precludes the discovery of new information, such as, in this case, new treatment options. People often use search engines to simultaneously find and re-find information. The Re:Search Engine is designed to support both behaviors in dynamic environments like the Web by preserving only the memorable aspects of a result list. A study of result list memory shows that people forget a lot. The Re:Search Engine takes advantage of these memory lapses to include new results where old results have been forgotten."	The re: search engine: simultaneous support for finding and re-finding.
7376	['J', 'Renaud Gervais', 'St', 'Fabien Lotte', 'Martin Hachet']	Teegi: tangible EEG interface.	301-308	2014	UIST	UIST	['https://doi.org/10.1145/2642918.2647368']	conf/uist/2014	['db/conf/uist/uist2014.html#FreyGFLH14']												We introduce Teegi, a Tangible ElectroEncephaloGraphy (EEG) Interface that enables novice users to get to know more about something as complex as brain signals, in an easy, engaging and informative way. To this end, we have designed a new system based on a unique combination of spatial augmented reality, tangible interaction and real-time neurotechnologies. With Teegi, a user can visualize and analyze his or her own brain activity in real-time, on a tangible character that can be easily manipulated, and with which it is possible to interact. An exploration study has shown that interacting with Teegi seems to be easy, motivating, reliable and informative. Overall, this suggests that Teegi is a promising and relevant training and mediation tool for the general public.	['Machine learning', 'Multimedia information systems', 'Information systems', 'Information systems applications', 'Computing methodologies', 'Human computer interaction (HCI)', 'Human-centered computing']	21	['J', 'Renaud Gervais', 'St', 'Fabien Lotte', 'Martin Hachet']	['Multimedia Information Systems', 'Computing Methodologies', 'Information Systems', 'Human-centered Computing', 'Information Systems Applications', 'Human Computer Interaction', 'Machine Learning']	21.0	We introduce Teegi, a Tangible ElectroEncephaloGraphy (EEG) Interface that enables novice users to get to know more about something as complex as brain signals, in an easy, engaging and informative way. To this end, we have designed a new system based on a unique combination of spatial augmented reality, tangible interaction and real-time neurotechnologies. With Teegi, a user can visualize and analyze his or her own brain activity in real-time, on a tangible character that can be easily manipulated, and with which it is possible to interact. An exploration study has shown that interacting with Teegi seems to be easy, motivating, reliable and informative. Overall, this suggests that Teegi is a promising and relevant training and mediation tool for the general public.	Teegi: tangible EEG interface.
7377	['Ruta Desai', 'James McCann', 'Stelian Coros']	Assembly-aware Design of Printable Electromechanical Devices.	457-472	2018	UIST	UIST	['https://doi.org/10.1145/3242587.3242655']	conf/uist/2018	['db/conf/uist/uist2018.html#DesaiMC18']												From smart toys and household appliances to personal robots, electromechanical devices play an increasingly important role in our daily lives. Rather than relying on gadgets that are mass-produced, our goal is to enable casual users to custom-design such devices based on their own needs and preferences. To this end, we present a computational design system that leverages the power of digital fabrication and the emergence of affordable electronics such as sensors and microcontrollers. The input to our system consists of a 3D representation of the desired device's shape, and a set of user-preferred off-the-shelf components. Based on this input, our method generates an optimized, 3D printable enclosure that can house the required components. To create these designs automatically, we formalize a new spatio-temporal model that captures the entire assembly process, including the placement of the components within the device, mounting structures and attachment strategies, the order in which components must be inserted, and collision-free assembly paths. Using this model as a technical core, we then leverage engineering design guidelines and efficient numerical techniques to optimize device designs. In a user study, which also highlights the challenges of designing such devices, we find our system to be effective in reducing the entry barriers faced by casual users in creating such devices. We further demonstrate the versatility of our approach by designing and fabricating three devices with diverse functionalities.	['Engineering', 'Graphical user interfaces', 'Physical sciences and engineering', 'Computer-aided design', 'Human computer interaction (HCI)', 'Applied computing', 'Human-centered computing', 'Interaction paradigms']	2	['Ruta Desai', 'James Mccann', 'Stelian Coros']	['Graphical User Interfaces', 'Physical Sciences And Engineering', 'Human-centered Computing', 'Computer-aided Design', 'Applied Computing', 'Human Computer Interaction', 'Engineering', 'Interaction Paradigms']	2.0	From smart toys and household appliances to personal robots, electromechanical devices play an increasingly important role in our daily lives. Rather than relying on gadgets that are mass-produced, our goal is to enable casual users to custom-design such devices based on their own needs and preferences. To this end, we present a computational design system that leverages the power of digital fabrication and the emergence of affordable electronics such as sensors and microcontrollers. The input to our system consists of a 3D representation of the desired device's shape, and a set of user-preferred off-the-shelf components. Based on this input, our method generates an optimized, 3D printable enclosure that can house the required components. To create these designs automatically, we formalize a new spatio-temporal model that captures the entire assembly process, including the placement of the components within the device, mounting structures and attachment strategies, the order in which components must be inserted, and collision-free assembly paths. Using this model as a technical core, we then leverage engineering design guidelines and efficient numerical techniques to optimize device designs. In a user study, which also highlights the challenges of designing such devices, we find our system to be effective in reducing the entry barriers faced by casual users in creating such devices. We further demonstrate the versatility of our approach by designing and fabricating three devices with diverse functionalities.	Assembly-aware Design of Printable Electromechanical Devices.
7378	['Sarah E. Chasins', 'Maria Mueller', 'Rastislav Bod']	Rousillon: Scraping Distributed Hierarchical Web Data.	963-975	2018	UIST	UIST	['https://doi.org/10.1145/3242587.3242661']	conf/uist/2018	['db/conf/uist/uist2018.html#ChasinsMB18']												Programming by Demonstration (PBD) promises to enable data scientists to collect web data. However, in formative interviews with social scientists, we learned that current PBD tools are insufficient for many real-world web scraping tasks. The missing piece is the capability to collect hierarchically-structured data from across many different webpages. We present Rousillon, a programming system for writing complex web automation scripts by demonstration. Users demonstrate how to collect the first row of a 'universal table' view of a hierarchical dataset to teach Rousillon how to collect all rows. To offer this new demonstration model, we developed novel relation selection and generalization algorithms. In a within-subject user study on 15 computer scientists, users can write hierarchical web scrapers 8 times more quickly with Rousillon than with traditional programming.	['Web-based interaction', 'World Wide Web', 'Software and its engineering', 'Development frameworks and environments', 'Data extraction and integration', 'Information systems', 'Web mining', 'Context specific languages', 'Programming by example', 'Integrated and visual development environments', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms', 'Software notations and tools']	17	['Sarah E. Chasins', 'Maria Mueller', 'Rastislav Bod']	['Software Notations And Tools', 'Information Systems', 'Human-centered Computing', 'Interaction Paradigms', 'Data Extraction And Integration', 'Development Frameworks And Environments', 'Web Mining', 'Programming By Example', 'World Wide Web', 'Context Specific Languages', 'Human Computer Interaction', 'Software And Its Engineering', 'Web-based Interaction', 'Integrated And Visual Development Environments']	17.0	Programming by Demonstration (PBD) promises to enable data scientists to collect web data. However, in formative interviews with social scientists, we learned that current PBD tools are insufficient for many real-world web scraping tasks. The missing piece is the capability to collect hierarchically-structured data from across many different webpages. We present Rousillon, a programming system for writing complex web automation scripts by demonstration. Users demonstrate how to collect the first row of a 'universal table' view of a hierarchical dataset to teach Rousillon how to collect all rows. To offer this new demonstration model, we developed novel relation selection and generalization algorithms. In a within-subject user study on 15 computer scientists, users can write hierarchical web scrapers 8 times more quickly with Rousillon than with traditional programming.	Rousillon: Scraping Distributed Hierarchical Web Data.
7379	['Thomas Augsten', 'Konstantin Kaefer', 'Ren', 'Caroline Fetzer', 'Dorian Kanitz', 'Thomas Stoff', 'Torsten Becker', 'Christian Holz 0001', 'Patrick Baudisch']	Multitoe: high-precision interaction with back-projected floors based on high-resolution multi-touch input.	209-218	2010	UIST	UIST	['https://doi.org/10.1145/1866029.1866064']	conf/uist/2010	['db/conf/uist/uist2010.html#AugstenKMFKSBHB10']												Tabletop applications cannot display more than a few dozen on-screen objects. The reason is their limited size: tables cannot become larger than arm's length without giving up direct touch. We propose creating direct touch surfaces that are orders of magnitude larger. We approach this challenge by integrating high-resolution multitouch input into a back-projected floor. As the same time, we maintain the purpose and interaction concepts of tabletop computers, namely direct manipulation.	['Touch screens', 'Interaction devices', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms']	70	['Thomas Augsten', 'Konstantin Kaefer', 'Ren', 'Caroline Fetzer', 'Dorian Kanitz', 'Thomas Stoff', 'Torsten Becker', 'Christian Holz 0001', 'Patrick Baudisch']	['Touch Screens', 'Human-centered Computing', 'Interaction Devices', 'Human Computer Interaction', 'Interaction Paradigms']	70.0	Tabletop applications cannot display more than a few dozen on-screen objects. The reason is their limited size: tables cannot become larger than arm's length without giving up direct touch. We propose creating direct touch surfaces that are orders of magnitude larger. We approach this challenge by integrating high-resolution multitouch input into a back-projected floor. As the same time, we maintain the purpose and interaction concepts of tabletop computers, namely direct manipulation.	Multitoe: high-precision interaction with back-projected floors based on high-resolution multi-touch input.
7380	['Jeremy Warner', 'Ben Lafreniere', 'George W. Fitzmaurice', 'Tovi Grossman']	ElectroTutor: Test-Driven Physical Computing Tutorials.	435-446	2018	UIST	UIST	['https://doi.org/10.1145/3242587.3242591']	conf/uist/2018	['db/conf/uist/uist2018.html#WarnerLFG18']												A wide variety of tools for creating physical computing systems have been developed, but getting started in this domain remains challenging for novices. In this paper, we introduce test-driven physical computing tutorials, a novel application of interactive tutorial systems to better support users in building and programming physical computing systems. These tutorials inject interactive tests into the tutorial process to help users verify and understand individual steps before proceeding. We begin by presenting a taxonomy of the types of tests that can be incorporated into physical computing tutorials. We then present ElectroTutor, a tutorial system that implements a range of tests for both the software and physical aspects of a physical computing system. A user study suggests that ElectroTutor can improve users' success and confidence when completing a tutorial, and save them time by reducing the need to backtrack and troubleshoot errors made on previous tutorial steps.	['Graphical user interfaces', 'Interaction design', 'Interactive systems and tools', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms']	7	['Jeremy Warner', 'Ben Lafreniere', 'George W. Fitzmaurice', 'Tovi Grossman']	['Graphical User Interfaces', 'Interaction Design', 'Human-centered Computing', 'Interactive Systems And Tools', 'Human Computer Interaction', 'Interaction Paradigms']	7.0	A wide variety of tools for creating physical computing systems have been developed, but getting started in this domain remains challenging for novices. In this paper, we introduce test-driven physical computing tutorials, a novel application of interactive tutorial systems to better support users in building and programming physical computing systems. These tutorials inject interactive tests into the tutorial process to help users verify and understand individual steps before proceeding. We begin by presenting a taxonomy of the types of tests that can be incorporated into physical computing tutorials. We then present ElectroTutor, a tutorial system that implements a range of tests for both the software and physical aspects of a physical computing system. A user study suggests that ElectroTutor can improve users' success and confidence when completing a tutorial, and save them time by reducing the need to backtrack and troubleshoot errors made on previous tutorial steps.	ElectroTutor: Test-Driven Physical Computing Tutorials.
7381	['Thomas Langerak', 'Juan Jos', 'Velko Vechev', 'David Lindlbauer', 'Daniele Panozzo', 'Otmar Hilliges']	Optimal Control for Electromagnetic Haptic Guidance Systems.	951-965	2020	UIST	UIST	['https://doi.org/10.1145/3379337.3415593']	conf/uist/2020	['db/conf/uist/uist2020.html#LangerakZVLPH20']												We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.	['Communication hardware', 'Hardware', 'Interaction devices', 'Tactile and hand-based interfaces', 'Haptic devices', 'Human computer interaction (HCI)', 'Human-centered computing']	1	['Thomas Langerak', 'Juan Jos', 'Velko Vechev', 'David Lindlbauer', 'Daniele Panozzo', 'Otmar Hilliges']	['Human-centered Computing', 'Hardware', 'Tactile And Hand-based Interfaces', 'Communication Hardware', 'Interaction Devices', 'Human Computer Interaction', 'Haptic Devices']	1.0	We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.	Optimal Control for Electromagnetic Haptic Guidance Systems.
7383	['Jan Wielemaker', 'Anjo Anjewierden']	Separating user interface and functionality using a frame based data model.	25-33	1989	UIST	UIST	['https://doi.org/10.1145/73660.73664']	conf/uist/1989	['db/conf/uist/uist1989.html#WielemakerA89']												The separation between user interface and functionality found in many screen editors is generalized to handle a data model based on frames and binary relations. This paper describes a User Interface Management System (UIMS) based on the data model. The UIMS is capable of maintaining different and simultaneous representations of the same application data objects. The functionality and user interface are implemented on top of a small object oriented programming system. This allows the UIMS to be simple and independent of the graphics software and hardware as well as the data representation used by the application programs.	['Professional topics', 'System management', 'Software and its engineering', 'Development frameworks and environments', 'Social and professional topics', 'Management of computing and information systems', 'Integrated and visual development environments', 'File systems management', 'Software notations and tools']	4	['Jan Wielemaker', 'Anjo Anjewierden']	['Software Notations And Tools', 'Professional Topics', 'File Systems Management', 'System Management', 'Social And Professional Topics', 'Development Frameworks And Environments', 'Management Of Computing And Information Systems', 'Software And Its Engineering', 'Integrated And Visual Development Environments']	4.0	The separation between user interface and functionality found in many screen editors is generalized to handle a data model based on frames and binary relations. This paper describes a User Interface Management System (UIMS) based on the data model. The UIMS is capable of maintaining different and simultaneous representations of the same application data objects. The functionality and user interface are implemented on top of a small object oriented programming system. This allows the UIMS to be simple and independent of the graphics software and hardware as well as the data representation used by the application programs.	Separating user interface and functionality using a frame based data model.
7384	['Shwetak N. Patel', 'Gregory D. Abowd']	Blui: low-cost localized blowable user interfaces.	217-220	2007	UIST	UIST	['https://doi.org/10.1145/1294211.1294250']	conf/uist/2007	['db/conf/uist/uist2007.html#PatelA07']												We describe a unique form of hands-free interaction that can be implemented on most commodity computing platforms. Our approach supports blowing at a laptop or computer screen to directly control certain interactive applications. Localization estimates are produced in real-time to determine where on the screen the person is blowing. Our approach relies solely on a single microphone, such as those already embedded in a standard laptop or one placed near a computer monitor, which makes our approach very cost-effective and easy-to-deploy. We show example interaction techniques that leverage this approach.	['Graphical user interfaces', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms']	28	['Shwetak N. Patel', 'Gregory D. Abowd']	['Graphical User Interfaces', 'Human Computer Interaction', 'Interaction Paradigms', 'Human-centered Computing']	28.0	We describe a unique form of hands-free interaction that can be implemented on most commodity computing platforms. Our approach supports blowing at a laptop or computer screen to directly control certain interactive applications. Localization estimates are produced in real-time to determine where on the screen the person is blowing. Our approach relies solely on a single microphone, such as those already embedded in a standard laptop or one placed near a computer monitor, which makes our approach very cost-effective and easy-to-deploy. We show example interaction techniques that leverage this approach.	Blui: low-cost localized blowable user interfaces.
7385	['Paul Strohmeier', 'Seref G', 'Luis Herres', 'Dennis Gudea', 'Bruno Fruchard', 'J']	bARefoot: Generating Virtual Materials using Motion Coupled Vibration in Shoes.	579-593	2020	UIST	UIST	['https://doi.org/10.1145/3379337.3415828']	conf/uist/2020	['db/conf/uist/uist2020.html#StrohmeierGHGFS20']												Many features of materials can be experienced through tactile cues, even using one's feet. For example, one can easily distinguish between moss and stone without looking at the ground. However, this type of material experience is largely not supported in AR and VR applications. We present bARefoot, a prototype shoe providing tactile impulses tightly coupled to motor actions. This enables generating virtual material experiences such as compliance, elasticity, or friction. To explore the parameter space of such sensorimotor coupled vibrations, we present a design tool enabling rapid design of virtual materials. We report initial explorations to increase understanding of how parameters can be optimized for generating compliance, and to examine the effect of dynamic parameters on material experiences. Finally, we present a series of use cases that demonstrate the potential of bARefoot for VR and AR.	['Human computer interaction (HCI)', 'Human-centered computing', 'Interaction devices', 'Haptic devices']	0	['Paul Strohmeier', 'Seref G', 'Luis Herres', 'Dennis Gudea', 'Bruno Fruchard', 'J']	['Interaction Devices', 'Human Computer Interaction', 'Haptic Devices', 'Human-centered Computing']	0.0	Many features of materials can be experienced through tactile cues, even using one's feet. For example, one can easily distinguish between moss and stone without looking at the ground. However, this type of material experience is largely not supported in AR and VR applications. We present bARefoot, a prototype shoe providing tactile impulses tightly coupled to motor actions. This enables generating virtual material experiences such as compliance, elasticity, or friction. To explore the parameter space of such sensorimotor coupled vibrations, we present a design tool enabling rapid design of virtual materials. We report initial explorations to increase understanding of how parameters can be optimized for generating compliance, and to examine the effect of dynamic parameters on material experiences. Finally, we present a series of use cases that demonstrate the potential of bARefoot for VR and AR.	bARefoot: Generating Virtual Materials using Motion Coupled Vibration in Shoes.
7386	['Andreas Paepcke', 'Bianca Soto', 'Leila Takayama', 'Frank Koenig', 'Blaise Gassend']	Yelling in the hall: using sidetone to address a problem with mobile remote presence systems.	107-116	2011	UIST	UIST	['https://doi.org/10.1145/2047196.2047209']	conf/uist/2011	['db/conf/uist/uist2011.html#PaepckeSTKG11']												In our field deployments of mobile remote presence (MRP) systems in offices, we observed that remote operators of MRPs often unintentionally spoke too loudly. This disrupted their local co-workers, who happened to be within earshot of the MRP system. To address this issue, we prototyped and empirically evaluated the effect of sidetone to help operators self regulate their speaking loudness. Sidetone is the intentional, attenuated feedback of speakers' voices to their ears while they are using a telecommunication device. In a 3-level (no sidetone vs. low sidetone vs. high sidetone) within- participants pair of experiments, people interacted with a confederate through an MRP system. The first experiment involved MRP operators using headsets with boom microphones (N=20). The second experiment involved MRP operators using loudspeakers and desktop microphones (N=14). While we detected the effects of the sidetone manipulation in our audio-visual context, the effect was attenuated in comparison to earlier audio-only studies. We hypothesize that the strong visual component of our MRP system interferes with the sidetone effect. We also found that engaging in more social tasks (e.g., a getting-to-know-you activity) and more intellectually demanding tasks (e.g., a creativity exercise) influenced how loudly people spoke. This suggests that testing such sidetone effects in the typical read-aloud setting is insufficient for generalizing to more interactive, communication tasks. We conclude that MRP application support must reach beyond the time honored audio-only technologies to solve the problem of excessive speaker loudness.	['Interaction techniques', 'Human computer interaction (HCI)', 'Auditory feedback', 'Human-centered computing']	15	['Andreas Paepcke', 'Bianca Soto', 'Leila Takayama', 'Frank Koenig', 'Blaise Gassend']	['Human Computer Interaction', 'Human-centered Computing', 'Auditory Feedback', 'Interaction Techniques']	15.0	In our field deployments of mobile remote presence (MRP) systems in offices, we observed that remote operators of MRPs often unintentionally spoke too loudly. This disrupted their local co-workers, who happened to be within earshot of the MRP system. To address this issue, we prototyped and empirically evaluated the effect of sidetone to help operators self regulate their speaking loudness. Sidetone is the intentional, attenuated feedback of speakers' voices to their ears while they are using a telecommunication device. In a 3-level (no sidetone vs. low sidetone vs. high sidetone) within- participants pair of experiments, people interacted with a confederate through an MRP system. The first experiment involved MRP operators using headsets with boom microphones (N=20). The second experiment involved MRP operators using loudspeakers and desktop microphones (N=14). While we detected the effects of the sidetone manipulation in our audio-visual context, the effect was attenuated in comparison to earlier audio-only studies. We hypothesize that the strong visual component of our MRP system interferes with the sidetone effect. We also found that engaging in more social tasks (e.g., a getting-to-know-you activity) and more intellectually demanding tasks (e.g., a creativity exercise) influenced how loudly people spoke. This suggests that testing such sidetone effects in the typical read-aloud setting is insufficient for generalizing to more interactive, communication tasks. We conclude that MRP application support must reach beyond the time honored audio-only technologies to solve the problem of excessive speaker loudness.	Yelling in the hall: using sidetone to address a problem with mobile remote presence systems.
7387	['Clifton Forlines', 'Daniel Vogel 0001', 'Ravin Balakrishnan']	HybridPointing: fluid switching between absolute and relative pointing with a direct input device.	211-220	2006	UIST	UIST	['https://doi.org/10.1145/1166253.1166286']	conf/uist/2006	['db/conf/uist/uist2006.html#ForlinesVB06']												"We present HybridPointing, a technique that lets users easily switch between absolute and relative pointing with a direct input device such as a pen. Our design includes a new graphical element, the Trailing Widget, which remains ""close at hand"" but does not interfere with normal cursor operation. The use of visual feedback to aid the user's understanding of input state is discussed, and several novel visual aids are presented. An experiment conducted on a large, wall-sized display validates the benefits of HybridPointing under certain conditions. We also discuss other situations in which HybridPointing may be useful. Finally, we present an extension to our technique that allows for switching between absolute and relative input in the middle of a single drag-operation."	['Graphical user interfaces', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms']	77	['Clifton Forlines', 'Daniel Vogel 0001', 'Ravin Balakrishnan']	['Graphical User Interfaces', 'Human Computer Interaction', 'Interaction Paradigms', 'Human-centered Computing']	77.0	"We present HybridPointing, a technique that lets users easily switch between absolute and relative pointing with a direct input device such as a pen. Our design includes a new graphical element, the Trailing Widget, which remains ""close at hand"" but does not interfere with normal cursor operation. The use of visual feedback to aid the user's understanding of input state is discussed, and several novel visual aids are presented. An experiment conducted on a large, wall-sized display validates the benefits of HybridPointing under certain conditions. We also discuss other situations in which HybridPointing may be useful. Finally, we present an extension to our technique that allows for switching between absolute and relative input in the middle of a single drag-operation."	HybridPointing: fluid switching between absolute and relative pointing with a direct input device.
7388	['Jessalyn Alvina', 'Joseph W. Malloch', 'Wendy E. Mackay']	Expressive Keyboards: Enriching Gesture-Typing on Mobile Devices.	583-593	2016	UIST	UIST	['https://doi.org/10.1145/2984511.2984560']	conf/uist/2016	['db/conf/uist/uist2016.html#AlvinaMM16']												"Gesture-typing is an efficient, easy-to-learn, and errortolerant technique for entering text on software keyboards. Our goal is to ""recycle"" users' otherwise-unused gesture variation to create rich output under the users' control, without sacrificing accuracy. Experiment 1 reveals a high level of existing gesture variation, even for accurate text, and shows that users can consciously vary their gestures under different conditions. We designed an Expressive Keyboard for a smart phone which maps input gesture features identified in Experiment 1 to a continuous output parameter space, i.e. RGB color. Experiment 2 shows that users can consciously modify their gestures, while retaining accuracy, to generate specific colors as they gesture-type. Users are more successful when they focus on output characteristics (such as red) rather than input characteristics (such as curviness). We designed an app with a dynamic font engine that continuously interpolates between several typefaces, as well as controlling weight and random variation. Experiment 3 shows that, in the context of a more ecologically-valid conversation task, users enjoy generating multiple forms of rich output. We conclude with suggestions for how the Expressive Keyboard approach can enhance a wide variety of gesture recognition applications."	['Human computer interaction (HCI)', 'Human-centered computing']	9	['Jessalyn Alvina', 'Joseph W. Malloch', 'Wendy E. Mackay']	['Human Computer Interaction', 'Human-centered Computing']	9.0	"Gesture-typing is an efficient, easy-to-learn, and errortolerant technique for entering text on software keyboards. Our goal is to ""recycle"" users' otherwise-unused gesture variation to create rich output under the users' control, without sacrificing accuracy. Experiment 1 reveals a high level of existing gesture variation, even for accurate text, and shows that users can consciously vary their gestures under different conditions. We designed an Expressive Keyboard for a smart phone which maps input gesture features identified in Experiment 1 to a continuous output parameter space, i.e. RGB color. Experiment 2 shows that users can consciously modify their gestures, while retaining accuracy, to generate specific colors as they gesture-type. Users are more successful when they focus on output characteristics (such as red) rather than input characteristics (such as curviness). We designed an app with a dynamic font engine that continuously interpolates between several typefaces, as well as controlling weight and random variation. Experiment 3 shows that, in the context of a more ecologically-valid conversation task, users enjoy generating multiple forms of rich output. We conclude with suggestions for how the Expressive Keyboard approach can enhance a wide variety of gesture recognition applications."	Expressive Keyboards: Enriching Gesture-Typing on Mobile Devices.
7389	['Fabrizio Pece', 'Juan Jose Zarate', 'Velko Vechev', 'Nadine Besse', 'Olexandr Gudozhnik', 'Herbert Shea', 'Otmar Hilliges']	MagTics: Flexible and Thin Form Factor Magnetic Actuators for Dynamic and Wearable Haptic Feedback.	143-154	2017	UIST	UIST	['https://doi.org/10.1145/3126594.3126609']	conf/uist/2017	['db/conf/uist/uist2017.html#PeceZVBGSH17']												We present MagTics, a novel flexible and wearable haptic interface based on magnetically actuated bidirectional tactile pixels (taxels). MagTics' thin form factor and flexibility allows for rich haptic feedback in mobile settings. We propose a novel actuation mechanism based on bistable electromagnetic latching that combines high frame rate and holding force with low energy consumption and a soft and flexible form factor. We overcome limitations of traditional soft actuators by placing several hard actuation cells, driven by flexible printed electronics, in a soft 3D printed case. A novel EM-shielding prevents magnet-magnet interactions and allows for high actuator densities. A prototypical implementation comprising of 4 actuated pins on a 1.7 cm pitch, with 2 mm travel, and generating 160 mN to 200 mN of latching force is used to implement a number of compelling application scenarios including adding haptic and tactile display capabilities to wearable devices, to existing input devices and to provide localized haptic feedback in virtual reality. Finally, we report results of a psychophysical study, conducted to inform future developments and to identify possible application domains.	['Human computer interaction (HCI)', 'Human-centered computing']	19	['Fabrizio Pece', 'Juan Jose Zarate', 'Velko Vechev', 'Nadine Besse', 'Olexandr Gudozhnik', 'Herbert Shea', 'Otmar Hilliges']	['Human Computer Interaction', 'Human-centered Computing']	19.0	We present MagTics, a novel flexible and wearable haptic interface based on magnetically actuated bidirectional tactile pixels (taxels). MagTics' thin form factor and flexibility allows for rich haptic feedback in mobile settings. We propose a novel actuation mechanism based on bistable electromagnetic latching that combines high frame rate and holding force with low energy consumption and a soft and flexible form factor. We overcome limitations of traditional soft actuators by placing several hard actuation cells, driven by flexible printed electronics, in a soft 3D printed case. A novel EM-shielding prevents magnet-magnet interactions and allows for high actuator densities. A prototypical implementation comprising of 4 actuated pins on a 1.7 cm pitch, with 2 mm travel, and generating 160 mN to 200 mN of latching force is used to implement a number of compelling application scenarios including adding haptic and tactile display capabilities to wearable devices, to existing input devices and to provide localized haptic feedback in virtual reality. Finally, we report results of a psychophysical study, conducted to inform future developments and to identify possible application domains.	MagTics: Flexible and Thin Form Factor Magnetic Actuators for Dynamic and Wearable Haptic Feedback.
7390	['Paul H. Dietz', 'Benjamin D. Eidelson', 'Jonathan Westhues', 'Steven Bathiche']	A practical pressure sensitive computer keyboard.	55-58	2009	UIST	UIST	['https://doi.org/10.1145/1622176.1622187']	conf/uist/2009	['db/conf/uist/uist2009.html#DietzEWB09']												A pressure sensitive computer keyboard is presented that independently senses the force level on every depressed key. The design leverages existing membrane technologies and is suitable for low-cost, high-volume manufacturing. A number of representative applications are discussed.	['Touch screens', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction devices']	39	['Paul H. Dietz', 'Benjamin D. Eidelson', 'Jonathan Westhues', 'Steven Bathiche']	['Interaction Devices', 'Human Computer Interaction', 'Touch Screens', 'Human-centered Computing']	39.0	A pressure sensitive computer keyboard is presented that independently senses the force level on every depressed key. The design leverages existing membrane technologies and is suitable for low-cost, high-volume manufacturing. A number of representative applications are discussed.	A practical pressure sensitive computer keyboard.
7391	['Richard Mortier', 'Tom Rodden', 'Peter Tolmie', 'Tom Lodge', 'Robert Spencer', 'Andy Crabtree', 'Joe Sventek', 'Alexandros Koliousis']	Homework: putting interaction into the infrastructure.	197-206	2012	UIST	UIST	['https://doi.org/10.1145/2380116.2380143']	conf/uist/2012	['db/conf/uist/uist2012.html#MortierRTLSCSK12']												This paper presents a user driven redesign of the domestic network infrastructure that draws upon a series of ethnographic studies of home networks. We present an infrastructure based around a purpose built access point that has modified the handling of protocols and services to reflect the interactive needs of the home. The developed infrastructure offers a novel measurement framework that allows a broad range of infrastructure information to be easily captured and made available to interactive applications. This is complemented by a diverse set of novel interactive control mechanisms and interfaces for the underlying infrastructure. We also briefly reflect on the technical and user issues arising from deployments.	['Human computer interaction (HCI)', 'Human-centered computing']	19	['Richard Mortier', 'Tom Rodden', 'Peter Tolmie', 'Tom Lodge', 'Robert Spencer', 'Andy Crabtree', 'Joe Sventek', 'Alexandros Koliousis']	['Human Computer Interaction', 'Human-centered Computing']	19.0	This paper presents a user driven redesign of the domestic network infrastructure that draws upon a series of ethnographic studies of home networks. We present an infrastructure based around a purpose built access point that has modified the handling of protocols and services to reflect the interactive needs of the home. The developed infrastructure offers a novel measurement framework that allows a broad range of infrastructure information to be easily captured and made available to interactive applications. This is complemented by a diverse set of novel interactive control mechanisms and interfaces for the underlying infrastructure. We also briefly reflect on the technical and user issues arising from deployments.	Homework: putting interaction into the infrastructure.
7392	['Kento Miyaoku', 'Suguru Higashino', 'Yoshinobu Tonomura']	C-blink: a hue-difference-based light signal marker for large screen interaction via any mobile terminal.	147-156	2004	UIST	UIST	['https://doi.org/10.1145/1029632.1029657']	conf/uist/2004	['db/conf/uist/uist2004.html#MiyaokuHT04']												"To enable common mobile terminals to interact with contents shown on large screens, we propose ""C-Blink"", a new light signal marker method that uses the color liquid-crystal display of a mobile terminal as a visible light source. We overcome the performance limitations of such displays by developing a hue-difference-blink technique. In combination with a screen-side sensor, we describe a system that detects and receives light signal markers sent by cell phone displays. Evaluations of a prototype system confirm that C-Blink performs well under common indoor lighting. The C-Blink program can be installed in any mobile terminal that has a color display, and the installation costs are small. C-Blink is a very useful way of enabling ubiquitous large screens to become interfaces for mobile terminals."	['Communication hardware', 'Touch screens', 'Graphics systems and interfaces', 'Signal processing systems', 'Computer vision', 'Interaction devices', 'Computing methodologies', 'Computer graphics', 'Artificial intelligence', 'Hardware', 'Graphics input devices', 'Human computer interaction (HCI)', 'Human-centered computing']	40	['Kento Miyaoku', 'Suguru Higashino', 'Yoshinobu Tonomura']	['Graphics Input Devices', 'Touch Screens', 'Computing Methodologies', 'Computer Vision', 'Artificial Intelligence', 'Human-centered Computing', 'Hardware', 'Graphics Systems And Interfaces', 'Communication Hardware', 'Computer Graphics', 'Signal Processing Systems', 'Interaction Devices', 'Human Computer Interaction']	40.0	"To enable common mobile terminals to interact with contents shown on large screens, we propose ""C-Blink"", a new light signal marker method that uses the color liquid-crystal display of a mobile terminal as a visible light source. We overcome the performance limitations of such displays by developing a hue-difference-blink technique. In combination with a screen-side sensor, we describe a system that detects and receives light signal markers sent by cell phone displays. Evaluations of a prototype system confirm that C-Blink performs well under common indoor lighting. The C-Blink program can be installed in any mobile terminal that has a color display, and the installation costs are small. C-Blink is a very useful way of enabling ubiquitous large screens to become interfaces for mobile terminals."	C-blink: a hue-difference-based light signal marker for large screen interaction via any mobile terminal.
7393	['Florian Block', 'Daniel Wigdor', 'Brenda Caldwell Phillips', 'Michael S. Horn', 'Chia Shen']	FlowBlocks: a multi-touch ui for crowd interaction.	497-508	2012	UIST	UIST	['https://doi.org/10.1145/2380116.2380178']	conf/uist/2012	['db/conf/uist/uist2012.html#BlockWPHS12']												Multi-touch technology lends itself to collaborative crowd interaction (CI). However, common tap-operated widgets are impractical for CI, since they are susceptible to accidental touches and interference from other users. We present a novel multi-touch interface called FlowBlocks in which every UI action is invoked through a small sequence of user actions: dragging parametric UI-Blocks, and dropping them over operational UI-Docks. The FlowBlocks approach is advantageous for CI because it a) makes accidental touches inconsequential; and b) introduces design parameters for mutual awareness, concurrent input, and conflict management. FlowBlocks was successfully used on the floor of a busy natural history museum. We present the complete design space and describe a year-long iterative design and evaluation process which employed the Rapid Iterative Test and Evaluation (RITE) method in a museum setting.	['Human computer interaction (HCI)', 'Human-centered computing']	9	['Florian Block', 'Daniel Wigdor', 'Brenda Caldwell Phillips', 'Michael S. Horn', 'Chia Shen']	['Human Computer Interaction', 'Human-centered Computing']	9.0	Multi-touch technology lends itself to collaborative crowd interaction (CI). However, common tap-operated widgets are impractical for CI, since they are susceptible to accidental touches and interference from other users. We present a novel multi-touch interface called FlowBlocks in which every UI action is invoked through a small sequence of user actions: dragging parametric UI-Blocks, and dropping them over operational UI-Docks. The FlowBlocks approach is advantageous for CI because it a) makes accidental touches inconsequential; and b) introduces design parameters for mutual awareness, concurrent input, and conflict management. FlowBlocks was successfully used on the floor of a busy natural history museum. We present the complete design space and describe a year-long iterative design and evaluation process which employed the Rapid Iterative Test and Evaluation (RITE) method in a museum setting.	FlowBlocks: a multi-touch ui for crowd interaction.
7395	['Rorik Henrikson', 'Bruno Rodrigues De Ara', 'Fanny Chevalier', 'Karan Singh', 'Ravin Balakrishnan']	Multi-Device Storyboards for Cinematic Narratives in VR.	787-796	2016	UIST	UIST	['https://doi.org/10.1145/2984511.2984539']	conf/uist/2016	['db/conf/uist/uist2016.html#HenriksonACSB16']												Virtual Reality (VR) narratives have the unprecedented potential to connect with an audience through presence, placing viewers within the narrative. The onset of consumer VR has resulted in an explosion of interest in immersive storytelling. Planning narratives for VR, however, is a grand challenge due to its unique affordances, its evolving cinematic vocabulary, and most importantly the lack of supporting tools to explore the creative process in VR.	['Human computer interaction (HCI)', 'Human-centered computing']	20	['Rorik Henrikson', 'Bruno Rodrigues De Ara', 'Fanny Chevalier', 'Karan Singh', 'Ravin Balakrishnan']	['Human Computer Interaction', 'Human-centered Computing']	20.0	Virtual Reality (VR) narratives have the unprecedented potential to connect with an audience through presence, placing viewers within the narrative. The onset of consumer VR has resulted in an explosion of interest in immersive storytelling. Planning narratives for VR, however, is a grand challenge due to its unique affordances, its evolving cinematic vocabulary, and most importantly the lack of supporting tools to explore the creative process in VR.	Multi-Device Storyboards for Cinematic Narratives in VR.
7396	['Takatoshi Yoshida', 'Xiaoyan Shen', 'Koichi Yoshino', 'Ken Nakagaki', 'Hiroshi Ishii 0001']	SCALE: Enhancing Force-based Interaction by Processing Load Data from Load Sensitive Modules.	901-911	2019	UIST	UIST	['https://doi.org/10.1145/3332165.3347935']	conf/uist/2019	['db/conf/uist/uist2019.html#YoshidaSYNI19']												SCALE provides a framework for load data from distributed load-sensitive modules for exploring force-based interaction. Force conveys not only the force vector itself but also rich information about activities, including way of touching, object location and body motion. Our system captures these interactions on a single pipeline of load data processing. Furthermore, we have expanded the interaction area from a flat 2D surface to 3D volume by building a mathematical framework, which enables us to capture the vertical height of a touch point. These technical invention opens broad applications, including general shape capturing and motion recognition. We have packaged the framework into a physical prototyping kit, and conducted a workshop with product designers to evaluate our system in practical scenarios.	['Human computer interaction (HCI)', 'Interaction design theory', 'Interaction design', 'Interactive systems and tools', 'User interface toolkits', 'Human-centered computing']	1	['Takatoshi Yoshida', 'Xiaoyan Shen', 'Koichi Yoshino', 'Ken Nakagaki', 'Hiroshi Ishii 0001']	['Interaction Design', 'Human-centered Computing', 'Interactive Systems And Tools', 'Interaction Design Theory', 'User Interface Toolkits', 'Human Computer Interaction']	1.0	SCALE provides a framework for load data from distributed load-sensitive modules for exploring force-based interaction. Force conveys not only the force vector itself but also rich information about activities, including way of touching, object location and body motion. Our system captures these interactions on a single pipeline of load data processing. Furthermore, we have expanded the interaction area from a flat 2D surface to 3D volume by building a mathematical framework, which enables us to capture the vertical height of a touch point. These technical invention opens broad applications, including general shape capturing and motion recognition. We have packaged the framework into a physical prototyping kit, and conducted a workshop with product designers to evaluate our system in practical scenarios.	SCALE: Enhancing Force-based Interaction by Processing Load Data from Load Sensitive Modules.
7397	['Benjamin J. Lafreniere', 'Tovi Grossman', 'Fraser Anderson', 'Justin Matejka', 'Heather Kerrick', 'Danil Nagy', 'Lauren Vasey', 'Evan Atherton', 'Nicholas Beirne', 'Marcelo H. Coelho', 'Nicholas Cote', 'Steven Li', 'Andy Nogueira', 'Long Nguyen', 'Tobias Schwinn', 'James Stoddart', 'David Thomasson', 'Ray Wang', 'Thomas White', 'David Benjamin', 'Maurice Conti', 'Achim Menges', 'George W. Fitzmaurice']	Crowdsourced Fabrication.	15-28	2016	UIST	UIST	['https://doi.org/10.1145/2984511.2984553', 'https://www.wikidata.org/entity/Q59889690']	conf/uist/2016	['db/conf/uist/uist2016.html#LafreniereGAMKN16']												In recent years, extensive research in the HCI literature has explored interactive techniques for digital fabrication. However, little attention in this body of work has examined how to involve and guide human workers in fabricating larger-scale structures. We propose a novel model of crowdsourced fabrication, in which a large number of workers and volunteers are guided through the process of building a pre-designed structure. The process is facilitated by an intelligent construction space capable of guiding individual workers and coordinating the overall build process. More specifically, we explore the use of smartwatches, indoor location sensing, and instrumented construction materials to provide real-time guidance to workers, coordinated by a foreman engine that manages the overall build process. We report on a three day deployment of our system to construct a 12-tall bamboo pavilion with assistance from more than one hundred volunteer workers, and reflect on observations and feedback collected during the exhibit.	['Human computer interaction (HCI)', 'Human-centered computing']	17	['Benjamin J. Lafreniere', 'Tovi Grossman', 'Fraser Anderson', 'Justin Matejka', 'Heather Kerrick', 'Danil Nagy', 'Lauren Vasey', 'Evan Atherton', 'Nicholas Beirne', 'Marcelo H. Coelho', 'Nicholas Cote', 'Steven Li', 'Andy Nogueira', 'Long Nguyen', 'Tobias Schwinn', 'James Stoddart', 'David Thomasson', 'Ray Wang', 'Thomas White', 'David Benjamin', 'Maurice Conti', 'Achim Menges', 'George W. Fitzmaurice']	['Human Computer Interaction', 'Human-centered Computing']	17.0	In recent years, extensive research in the HCI literature has explored interactive techniques for digital fabrication. However, little attention in this body of work has examined how to involve and guide human workers in fabricating larger-scale structures. We propose a novel model of crowdsourced fabrication, in which a large number of workers and volunteers are guided through the process of building a pre-designed structure. The process is facilitated by an intelligent construction space capable of guiding individual workers and coordinating the overall build process. More specifically, we explore the use of smartwatches, indoor location sensing, and instrumented construction materials to provide real-time guidance to workers, coordinated by a foreman engine that manages the overall build process. We report on a three day deployment of our system to construct a 12-tall bamboo pavilion with assistance from more than one hundred volunteer workers, and reflect on observations and feedback collected during the exhibit.	Crowdsourced Fabrication.
7398	['G', 'St', 'Matthieu Falce', 'St', 'Nicolas Roussel']	Looking through the Eye of the Mouse: A Simple Method for Measuring End-to-end Latency using an Optical Mouse.	629-636	2015	UIST	UIST	['https://doi.org/10.1145/2807442.2807454']	conf/uist/2015	['db/conf/uist/uist2015.html#CasiezCFHR15']												We present a simple method for measuring end-to-end latency in graphical user interfaces. The method works with most optical mice and allows accurate and real time latency measures up to 5 times per second. In addition, the technique allows easy insertion of probes at different places in the system I.e. mouse events listeners - to investigate the sources of latency. After presenting the measurement method and our methodology, we detail the measures we performed on different systems, toolkits and applications. Results show that latency is affected by the operating system and system load. Substantial differences are found between C++/GLUT and C++/Qt or Java/Swing implementations, as well as between web browsers.	['Human computer interaction (HCI)', 'Human-centered computing']	15	['G', 'St', 'Matthieu Falce', 'St', 'Nicolas Roussel']	['Human Computer Interaction', 'Human-centered Computing']	15.0	We present a simple method for measuring end-to-end latency in graphical user interfaces. The method works with most optical mice and allows accurate and real time latency measures up to 5 times per second. In addition, the technique allows easy insertion of probes at different places in the system I.e. mouse events listeners - to investigate the sources of latency. After presenting the measurement method and our methodology, we detail the measures we performed on different systems, toolkits and applications. Results show that latency is affected by the operating system and system load. Substantial differences are found between C++/GLUT and C++/Qt or Java/Swing implementations, as well as between web browsers.	Looking through the Eye of the Mouse: A Simple Method for Measuring End-to-end Latency using an Optical Mouse.
7399	['Ryo Suzuki', 'Rubaiat Habib Kazi', 'Li-Yi Wei', 'Stephen DiVerdi', 'Wilmot Li', 'Daniel Leithinger']	RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching.	166-181	2020	UIST	UIST	['https://doi.org/10.1145/3379337.3415892']	conf/uist/2020	['db/conf/uist/uist2020.html#SuzukiKWDLL20']												We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid-air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.	['Mixed / augmented reality', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms']	2	['Ryo Suzuki', 'Rubaiat Habib Kazi', 'Li-yi Wei', 'Stephen Diverdi', 'Wilmot Li', 'Daniel Leithinger']	['Human Computer Interaction', 'Mixed Augmented Reality', 'Interaction Paradigms', 'Human-centered Computing']	2.0	We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid-air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.	RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching.
7400	['Jeremy M. Wolfe']	Capturing the user's attention: insights from the study of human vision.	191-192	2007	UIST	UIST	['https://doi.org/10.1145/1294211.1294245', 'https://www.wikidata.org/entity/Q57742918']	conf/uist/2007	['db/conf/uist/uist2007.html#Wolfe07']												An effective user interface is a cooperative interaction between humans and their technology. For that interaction to work, it needs to recognize the limitations and exploit the strengths of both parties. In this talk, I will concentrate on the human side of the equation. What do we know about human visual perceptual abilities that might have an impact on the design of user interfaces? The world presents us with more information than we can process. Just try to read this abstract and the next piece of prose at the same time. We cope with this problem by using attentional mechanisms to select a subset of the input for further processing. An inter-face might be designed to .capture. attention, in order to induce a human to interact with it. Once the human is using an interface, that interface should .guide. the user.s atten-tion in an intelligent manner. In recent decades, many of the rules of attentional capture and guidance have been worked out in the laboratory. I will illustrate some of the basic principles. For example: Do some colors grab attention better than others? Are faces special? When and why do people fail to .see. things that are right in front of their eyes.	['Interaction design process and methods', 'Human-centered computing', 'User centered design', 'Interaction design']	0	['Jeremy M. Wolfe']	['User Centered Design', 'Interaction Design Process And Methods', 'Interaction Design', 'Human-centered Computing']	0.0	An effective user interface is a cooperative interaction between humans and their technology. For that interaction to work, it needs to recognize the limitations and exploit the strengths of both parties. In this talk, I will concentrate on the human side of the equation. What do we know about human visual perceptual abilities that might have an impact on the design of user interfaces? The world presents us with more information than we can process. Just try to read this abstract and the next piece of prose at the same time. We cope with this problem by using attentional mechanisms to select a subset of the input for further processing. An inter-face might be designed to .capture. attention, in order to induce a human to interact with it. Once the human is using an interface, that interface should .guide. the user.s atten-tion in an intelligent manner. In recent decades, many of the rules of attentional capture and guidance have been worked out in the laboratory. I will illustrate some of the basic principles. For example: Do some colors grab attention better than others? Are faces special? When and why do people fail to .see. things that are right in front of their eyes.	Capturing the user's attention: insights from the study of human vision.
7401	['Mark W. Newman', 'Mark S. Ackerman', 'Jungwoo Kim', 'Atul Prakash', 'Zhenan Hong', 'Jacob Mandel', 'Tao Dong']	Bringing the field into the lab: supporting capture and replay of contextual data for the design of context-aware applications.	105-108	2010	UIST	UIST	['https://doi.org/10.1145/1866029.1866048']	conf/uist/2010	['db/conf/uist/uist2010.html#NewmanAKPHMD10']												When designing context-aware applications, it is difficult to for designers in the studio or lab to envision the contextual conditions that will be encountered at runtime. Designers need a tool that can create/re-create naturalistic contextual states and transitions, so that they can evaluate an application under expected contexts. We have designed and developed RePlay: a system for capturing and playing back sensor traces representing scenarios of use. RePlay contributes to research on ubicomp design tools by embodying a structured approach to the capture and playback of contextual data. In particular, RePlay supports: capturing naturalistic data through Capture Probes, encapsulating scenarios of use through Episodes, and supporting exploratory manipulation of scenarios through Transforms. Our experiences using RePlay in internal design projects illustrate its potential benefits for ubicomp design.	['Interaction design process and methods', 'Human-centered computing', 'Interface design prototyping', 'Interaction design']	12	['Mark W. Newman', 'Mark S. Ackerman', 'Jungwoo Kim', 'Atul Prakash', 'Zhenan Hong', 'Jacob Mandel', 'Tao Dong']	['Interaction Design Process And Methods', 'Interaction Design', 'Interface Design Prototyping', 'Human-centered Computing']	12.0	When designing context-aware applications, it is difficult to for designers in the studio or lab to envision the contextual conditions that will be encountered at runtime. Designers need a tool that can create/re-create naturalistic contextual states and transitions, so that they can evaluate an application under expected contexts. We have designed and developed RePlay: a system for capturing and playing back sensor traces representing scenarios of use. RePlay contributes to research on ubicomp design tools by embodying a structured approach to the capture and playback of contextual data. In particular, RePlay supports: capturing naturalistic data through Capture Probes, encapsulating scenarios of use through Episodes, and supporting exploratory manipulation of scenarios through Transforms. Our experiences using RePlay in internal design projects illustrate its potential benefits for ubicomp design.	Bringing the field into the lab: supporting capture and replay of contextual data for the design of context-aware applications.
7402	['Walter S. Lasecki', 'Rachel Wesley', 'Jeffrey Nichols', 'Anand Kulkarni', 'James F. Allen', 'Jeffrey P. Bigham']	Chorus: a crowd-powered conversational assistant.	151-162	2013	UIST	UIST	['https://doi.org/10.1145/2501988.2502057']	conf/uist/2013	['db/conf/uist/uist2013.html#LaseckiWNKAB13']												Despite decades of research attempting to establish conversational interaction between humans and computers, the capabilities of automated conversational systems are still limited. In this paper, we introduce Chorus, a crowd-powered conversational assistant. When using Chorus, end users converse continuously with what appears to be a single conversational partner. Behind the scenes, Chorus leverages multiple crowd workers to propose and vote on responses. A shared memory space helps the dynamic crowd workforce maintain consistency, and a game-theoretic incentive mechanism helps to balance their efforts between proposing and voting. Studies with 12 end users and 100 crowd workers demonstrate that Chorus can provide accurate, topical responses, answering nearly 93% of user queries appropriately, and staying on-topic in over 95% of responses. We also observed that Chorus has advantages over pairing an end user with a single crowd worker and end users completing their own tasks in terms of speed, quality, and breadth of assistance. Chorus demonstrates a new future in which conversational assistants are made usable in the real world by combining human and machine intelligence, and may enable a useful new way of interacting with the crowds powering other systems.	['Information systems', 'Information systems applications']	70	['Walter S. Lasecki', 'Rachel Wesley', 'Jeffrey Nichols', 'Anand Kulkarni', 'James F. Allen', 'Jeffrey P. Bigham']	['Information Systems Applications', 'Information Systems']	70.0	Despite decades of research attempting to establish conversational interaction between humans and computers, the capabilities of automated conversational systems are still limited. In this paper, we introduce Chorus, a crowd-powered conversational assistant. When using Chorus, end users converse continuously with what appears to be a single conversational partner. Behind the scenes, Chorus leverages multiple crowd workers to propose and vote on responses. A shared memory space helps the dynamic crowd workforce maintain consistency, and a game-theoretic incentive mechanism helps to balance their efforts between proposing and voting. Studies with 12 end users and 100 crowd workers demonstrate that Chorus can provide accurate, topical responses, answering nearly 93% of user queries appropriately, and staying on-topic in over 95% of responses. We also observed that Chorus has advantages over pairing an end user with a single crowd worker and end users completing their own tasks in terms of speed, quality, and breadth of assistance. Chorus demonstrates a new future in which conversational assistants are made usable in the real world by combining human and machine intelligence, and may enable a useful new way of interacting with the crowds powering other systems.	Chorus: a crowd-powered conversational assistant.
7403	['Michael Wessely', 'Theophanis Tsandilas', 'Wendy E. Mackay']	Stretchis: Fabricating Highly Stretchable User Interfaces.	697-704	2016	UIST	UIST	['https://doi.org/10.1145/2984511.2984521']	conf/uist/2016	['db/conf/uist/uist2016.html#WesselyTM16']												Recent advances in materials science research allow production of highly stretchable sensors and displays. Such technologies, however, are still not accessible to non-expert makers. We present a novel and inexpensive fabrication method for creating Stretchis, highly stretchable user interfaces that combine sensing capabilities and visual output. We use Polydimethylsiloxan (PDMS) as the base material for a Stretchi and show how to embed stretchable touch and proximity sensors and stretchable electroluminescent displays. Stretchis can be ultra-thin (≈ 200μm), flexible, and fully customizable, enabling non-expert makers to add interaction to elastic physical objects, shape-changing surfaces, fabrics, and the human body. We demonstrate the usefulness of our approach with three application examples that range from ubiquitous computing to wearables and on-skin interaction.	['Human computer interaction (HCI)', 'Human-centered computing']	35	['Michael Wessely', 'Theophanis Tsandilas', 'Wendy E. Mackay']	['Human Computer Interaction', 'Human-centered Computing']	35.0	Recent advances in materials science research allow production of highly stretchable sensors and displays. Such technologies, however, are still not accessible to non-expert makers. We present a novel and inexpensive fabrication method for creating Stretchis, highly stretchable user interfaces that combine sensing capabilities and visual output. We use Polydimethylsiloxan (PDMS) as the base material for a Stretchi and show how to embed stretchable touch and proximity sensors and stretchable electroluminescent displays. Stretchis can be ultra-thin (≈ 200μm), flexible, and fully customizable, enabling non-expert makers to add interaction to elastic physical objects, shape-changing surfaces, fabrics, and the human body. We demonstrate the usefulness of our approach with three application examples that range from ubiquitous computing to wearables and on-skin interaction.	Stretchis: Fabricating Highly Stretchable User Interfaces.
7404	['Xiaojun Bi', 'Shumin Zhai']	Bayesian touch: a statistical criterion of target selection with finger touch.	51-60	2013	UIST	UIST	['https://doi.org/10.1145/2501988.2502058']	conf/uist/2013	['db/conf/uist/uist2013.html#BiZ13']												To improve the accuracy of target selection for finger touch, we conceptualize finger touch input as an uncertain process, and derive a statistical target selection criterion, Bayesian Touch Criterion, by combining the basic Bayes' rule of probability with the generalized dual Gaussian distribution hypothesis of finger touch. The Bayesian Touch Criterion selects the intended target as the candidate with the shortest Bayesian Touch Distance to the touch point, which is computed from the touch point to the target center distance and the target size. We give the derivation of the Bayesian Touch Criterion and its empirical evaluation with two experiments. The results showed that for 2-dimensional circular target selection, the Bayesian Touch Criterion is significantly more accurate than the commonly used Visual Boundary Criterion (i.e., a target is selected if and only if the touch point falls within its boundary) and its two variants.	['Human computer interaction (HCI)', 'Human-centered computing']	37	['Xiaojun Bi', 'Shumin Zhai']	['Human Computer Interaction', 'Human-centered Computing']	37.0	To improve the accuracy of target selection for finger touch, we conceptualize finger touch input as an uncertain process, and derive a statistical target selection criterion, Bayesian Touch Criterion, by combining the basic Bayes' rule of probability with the generalized dual Gaussian distribution hypothesis of finger touch. The Bayesian Touch Criterion selects the intended target as the candidate with the shortest Bayesian Touch Distance to the touch point, which is computed from the touch point to the target center distance and the target size. We give the derivation of the Bayesian Touch Criterion and its empirical evaluation with two experiments. The results showed that for 2-dimensional circular target selection, the Bayesian Touch Criterion is significantly more accurate than the commonly used Visual Boundary Criterion (i.e., a target is selected if and only if the touch point falls within its boundary) and its two variants.	Bayesian touch: a statistical criterion of target selection with finger touch.
7405	['Eric Lecolinet']	A molecular architecture for creating advanced GUIs.	135-144	2003	UIST	UIST	['https://doi.org/10.1145/964696.964711']	conf/uist/2003	['db/conf/uist/uist2003.html#Lecolinet03']												This paper presents a new GUI architecture for creating advanced interfaces. This model is based on a limited set of general principles that improve flexibility and provide capabilities for implementing information visualization techniques such as magic lenses, transparent tools or semantic zooming. This architecture also makes it possible to create multiple views and application-sharing systems (by sharing views on multiple computer screens) in a simple and uniform way and to handle bimanual interaction and multiple pointers. An experimental toolkit called Ubit was implemented to test the feasibility of this approach. It is based on a pseudo-declarative C++ API that tries to simplify GUI programming by providing a higher level of abstraction.	['Shape modeling', 'Image manipulation', 'Graphical user interfaces', 'Randomness', 'Rendering', 'Collaborative and social computing', 'Computing methodologies', 'Computer graphics', 'Theory of computation', 'Collaborative and social computing theory', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms']	28	['Eric Lecolinet']	['Graphical User Interfaces', 'Computing Methodologies', 'Human-centered Computing', 'Image Manipulation', 'Interaction Paradigms', 'Collaborative And Social Computing', 'Computer Graphics', 'Collaborative And Social Computing Theory', 'Randomness', 'Human Computer Interaction', 'Shape Modeling', 'Theory Of Computation', 'Rendering']	28.0	This paper presents a new GUI architecture for creating advanced interfaces. This model is based on a limited set of general principles that improve flexibility and provide capabilities for implementing information visualization techniques such as magic lenses, transparent tools or semantic zooming. This architecture also makes it possible to create multiple views and application-sharing systems (by sharing views on multiple computer screens) in a simple and uniform way and to handle bimanual interaction and multiple pointers. An experimental toolkit called Ubit was implemented to test the feasibility of this approach. It is based on a pseudo-declarative C++ API that tries to simplify GUI programming by providing a higher level of abstraction.	A molecular architecture for creating advanced GUIs.
7407	['Tiare M. Feuchtner', 'J']	Ownershift: Facilitating Overhead Interaction in Virtual Reality with an Ownership-Preserving Hand Space Shift.	31-43	2018	UIST	UIST	['https://doi.org/10.1145/3242587.3242594']	conf/uist/2018	['db/conf/uist/uist2018.html#Feuchtner018']												We present Ownershift, an interaction technique for easing overhead manipulation in virtual reality, while preserving the illusion that the virtual hand is the user's own hand. In contrast to previous approaches, this technique does not alter the mapping of the virtual hand position for initial reaching movements towards the target. Instead, the virtual hand space is only shifted gradually if interaction with the overhead target requires an extended amount of time. While users perceive their virtual hand as operating overhead, their physical hand moves gradually to a less strained position at waist level. We evaluated the technique in a user study and show that Ownershift significantly reduces the physical strain of overhead interactions, while only slightly reducing task performance and the sense of body ownership of the virtual hand.	['Virtual reality', 'Interaction techniques', 'Empirical studies in HCI', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms']	3	['Tiare M. Feuchtner', 'J']	['Virtual Reality', 'Human-centered Computing', 'Empirical Studies In Hci', 'Human Computer Interaction', 'Interaction Paradigms', 'Interaction Techniques']	3.0	We present Ownershift, an interaction technique for easing overhead manipulation in virtual reality, while preserving the illusion that the virtual hand is the user's own hand. In contrast to previous approaches, this technique does not alter the mapping of the virtual hand position for initial reaching movements towards the target. Instead, the virtual hand space is only shifted gradually if interaction with the overhead target requires an extended amount of time. While users perceive their virtual hand as operating overhead, their physical hand moves gradually to a less strained position at waist level. We evaluated the technique in a user study and show that Ownershift significantly reduces the physical strain of overhead interactions, while only slightly reducing task performance and the sense of body ownership of the virtual hand.	Ownershift: Facilitating Overhead Interaction in Virtual Reality with an Ownership-Preserving Hand Space Shift.
7408	['Johnny C. Lee', 'Paul H. Dietz', 'Dan Maynes-Aminzade', 'Ramesh Raskar', 'Scott E. Hudson']	Automatic projector calibration with embedded light sensors.	123-126	2004	UIST	UIST	['https://doi.org/10.1145/1029632.1029653']	conf/uist/2004	['db/conf/uist/uist2004.html#LeeDMRH04']												Projection technology typically places several constraints on the geometric relationship between the projector and the projection surface to obtain an undistorted, properly sized image. In this paper we describe a simple, robust, fast, and low-cost method for automatic projector calibration that eliminates many of these constraints. We embed light sensors in the target surface, project Gray-coded binary patterns to discover the sensor locations, and then prewarp the image to accurately fit the physical features of the projection surface. This technique can be expanded to automatically stitch multiple projectors, calibrate onto non-planar surfaces for object decoration, and provide a method for simple geometry acquisition.	['Virtual reality', 'Graphics systems and interfaces', 'Mixed / augmented reality', 'Interaction devices', 'Computing methodologies', 'Computer graphics', 'Graphics input devices', 'Human computer interaction (HCI)', 'Human-centered computing', 'Interaction paradigms']	66	['Johnny C. Lee', 'Paul H. Dietz', 'Dan Maynes-aminzade', 'Ramesh Raskar', 'Scott E. Hudson']	['Graphics Input Devices', 'Virtual Reality', 'Computing Methodologies', 'Human-centered Computing', 'Graphics Systems And Interfaces', 'Computer Graphics', 'Mixed Augmented Reality', 'Interaction Devices', 'Human Computer Interaction', 'Interaction Paradigms']	66.0	Projection technology typically places several constraints on the geometric relationship between the projector and the projection surface to obtain an undistorted, properly sized image. In this paper we describe a simple, robust, fast, and low-cost method for automatic projector calibration that eliminates many of these constraints. We embed light sensors in the target surface, project Gray-coded binary patterns to discover the sensor locations, and then prewarp the image to accurately fit the physical features of the projection surface. This technique can be expanded to automatically stitch multiple projectors, calibrate onto non-planar surfaces for object decoration, and provide a method for simple geometry acquisition.	Automatic projector calibration with embedded light sensors.
